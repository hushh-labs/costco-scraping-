# ğŸ›’ Costco Product Scraper

Scrapes Costco product info & images like a beast and saves everything in a single CSV. Fully automated pipeline. All you gotta do is drop a CSV with product URLs. That's it.

---

## ğŸ§¾ Folder Structure

```

Costco Scraping/
â”œâ”€â”€ main.py                  # Main pipeline to run all steps
â”œâ”€â”€ Product\_id\_generate.py  # Fills in random product IDs
â”œâ”€â”€ Image\_download.py       # Downloads product images from bfasset
â”œâ”€â”€ fill\_costco\_data.py     # Scrapes brand, title, price, description etc.
â”œâ”€â”€ config.py               # Auto-generated config file with shared vars
â”œâ”€â”€ costco-products.csv     # Your input/output CSV (must have product\_url column)
â”œâ”€â”€ images/                 # Folder where all images will be saved
â”œâ”€â”€ requirements.txt        # All required libraries
â””â”€â”€ README.md               # You're reading it

````

---

## ğŸ”§ Setup Instructions

### 1. ğŸ“‚ Move into Folder

Make sure all the `.py` files and your CSV file are in **one folder**.

---

### 2. ğŸ§  Prepare the CSV

Create a CSV file called:  
**`costco-products.csv`**

It **must** have at least this column:

| product_url |
|-------------|
| https://www.costco.com/... |
| https://www.costco.com/... |

You can leave the rest empty â€” the script fills them in.

---

### 3. ğŸ“ Create the `images` Folder

Create it manually:

```bash
mkdir images
````

Or right-click â†’ New Folder â†’ name it `images`.

---

### 4. ğŸ’» Install Dependencies (Recommended: in a virtual environment)

```bash
python -m venv venv
venv\Scripts\activate        # For Windows
# source venv/bin/activate  # For Mac/Linux

pip install -r requirements.txt
playwright install
```

---

## ğŸš€ Run the Pipeline

```bash
python main.py
```

The script will:

1. âœ… Auto-generate product IDs if missing
2. ğŸ“¸ Download bfasset product images
3. ğŸ“‹ Scrape product info (title, brand, price, description)
4. ğŸ§¾ Save all data back into `costco-products.csv`

---

## ğŸª£ Supabase Setup (Manual Step)

> Images will be **downloaded locally into the `images/` folder**, but for the final links to work (in `image_url`), you need to:

1. Create a **Supabase bucket** (e.g. `costco-products-scrapped`)
2. Upload everything from the `images/` folder to that bucket manually
3. Make sure the bucket is **public**
4. Update the **`BASE_URL`** in `main.py` to point to your Supabase public bucket URL:

   ```python
   BASE_URL = "https://<your-project-ref>.supabase.co/storage/v1/object/public/<your-bucket-name>"
   ```

Thatâ€™s how image URLs in the CSV will become valid online links.

---

## âš ï¸ Notes

* `costco-products.csv` should not be empty and must have valid URLs.
* The browser window will pop up â€” donâ€™t close it unless debugging.
* If you see timeout or `HTTP/2` errors, check your internet or increase timeout in `Image_download.py`.
* No rate-limiting bypass logic is added yet â€” you can add rotating proxies or delays if needed.

---

## ğŸ§ª Sample Output CSV

| product\_id  | product\_url | image\_url                                                     | brand | price | description         | ... |
| ------------ | ------------ | -------------------------------------------------------------- | ----- | ----- | ------------------- | --- |
| prod\_abc123 | https\://... | [https://supabase.co/.../1.png](https://supabase.co/.../1.png) | Nike  | \$89  | Sleek running shoes | ... |

---

## ğŸ§™â€â™‚ï¸ Dev Tip

You only need to update `main.py`'s config:

```python
CSV_FILE = "costco-products.csv"
OUTPUT_DIR = "images"
BASE_URL = "https://your-supabase-link"
```

Everything else is auto-managed. Go build that scraper empire ğŸš€

```

---

Let me know if you want a version that **auto-uploads images to Supabase too** with API keys and everything. Thatâ€™d be ğŸ”¥ next level.
```
